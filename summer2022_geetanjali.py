# -*- coding: utf-8 -*-
"""Summer2022-Geetanjali.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kl6ZDGoDca_JOaayuIWsC0BE_b5IQGeD

#Title: Algorithm Design for Automatized Process of Building a Medical Diseasee Checklist

Summer Intership Project

Geetanjali Sharma

Supervisor: Minhee Jun

Data: May 19 - Aug 10, 2022

Project Description: Designing a software system that detects notorious substances and harmful chemical compounds that can exist in work environments and threaten workers’ health conditions in a long term. The work environments information will be obtained in the Material Safety Data Sheets (MSDS).  The detected substances will be categorized to build a medical diagnosis checklist.

**Phase 1: **

In phase 1 we will be taking a MSDS pdf file as input and separating the 16 different sections and saving them in individual json files.

So, first of all we are installing PyPDF2. PyPDF2 is a python library used to manipulate pdf files.
"""

pip install PyPDF2

"""Then we will import the PyPDF2 library. The first function clean_text is just designed to remove unwanted string from a file. 

The second function myfunction1 takes the MSDS file as input, opens it, extracts the pages of the pdf file, separates the different sections (16) of the MSDS file and finally saves them in 16 different json files.
"""

import PyPDF2

def clean_text(section_name, substring): # clean_text function deleted any unwanted substring from a given string. The first input is the string that you want to clean and the second input is the substring that you want to delete.
    split_section = section_name.find(substring) #finding the index of the substring to be deleted 
    revised_section = section_name[:split_section] #using list slicing to remove the substring
    return revised_section

def myfunction1(msds_filename, cleanup_string): #this function takes a MSDS file and a substring that you want to remove from your file as inputs.
 
  # read the pdf file 'msds_filename'
  
  pdfFileObj = open(msds_filename, 'rb') #opening the pdf file 
  pdfReader = PyPDF2.PdfFileReader(pdfFileObj) 
  pages = pdfReader.numPages
  total_string = ""
  #extracting text from the pdf file and saving it as a string
  for i in range (0, pages):
    pageObj = pdfReader.getPage(i)
    text = pageObj.extractText()
    total_string = total_string + text
  pdfFileObj.close() 
  total_string = total_string.replace('\n', '').replace('\r', '')
  # print(total_string)

  #finding the starting indices of all the 16 sections of the MSDS file
  section1_start = total_string.find("1. IDENTIFICATION")
  section2_start = total_string.find("2. HAZARDS IDENTIFICATION")
  section3_start = total_string.find("3. COMPOSION/INFORMATION ON ING REDIENTS")
  section4_start = total_string.find("4. FIRST-AIDMEASURES")
  section5_start = total_string.find("5. FIRE FIGHTING MEASURE")
  section6_start = total_string.find("6. ACCIDENTAL RELEASE MEASURES")
  section7_start = total_string.find("7. HANDLING AND  STORAGE")
  section8_start = total_string.find("8. EXPOSURE CONTROLS/PERSONAL PROTECTION")
  section9_start = total_string.find("9. Physical and Chemical Properties")
  section10_start = total_string.find(" 10. STABILITY AND REACTIVITY")
  section11_start = total_string.find("11. Toxicological Information")
  section12_start = total_string.find("12. ECOLOGICAL INFORMATION")
  section13_start = total_string.find("13. Disposal Considerations")
  section14_start = total_string.find("14. TRANSPORT INFORMATI ON")
  section15_start = total_string.find("15. REGULATORY INFORMATION")
  section16_start = total_string.find("16. OTHER INFORMATION")

  #extracting the text for each section and saving it as json files
  # get Section 1
  section1_text = total_string[section1_start:section2_start]
  json1 = {'section1':section1_text}

  # Section 2
  section2_text = total_string[section2_start:section3_start]
  section2_revised = clean_text(section2_text, cleanup_string)
  json2 = {'section2':section2_revised}

  # Section 3
  section3_text = total_string[section3_start:section4_start]
  section3_revised = clean_text(section3_text, cleanup_string)
  json3 = {'section3':section3_revised}

  # Section 4
  section4_text = total_string[section4_start:section5_start]
  section4_revised = clean_text(section4_text, cleanup_string)
  json4 = {'section4':section4_revised}

  # Section 5
  section5_text = total_string[section5_start:section6_start]
  section5_revised = clean_text(section5_text, cleanup_string)
  json5 = {'section5':section5_revised}

  # Section 6
  section6_text = total_string[section6_start:section7_start]
  section6_revised = clean_text(section6_text, cleanup_string)
  json6 = {'section6':section6_revised}

  # Section 7 
  section7_text = total_string[section7_start:section8_start]
  section7_revised = clean_text(section7_text, cleanup_string)
  json7 = {'section7':section7_revised}

  # Section 8
  section8_text = total_string[section8_start:section9_start]
  section8_revised = clean_text(section8_text, cleanup_string)
  json8 = {'section8':section8_revised}

  # Section 9
  section9_text = total_string[section9_start:section10_start]
  section9_revised = clean_text(section9_text, cleanup_string)
  json9 = {'section9':section9_revised}

  # Section 10
  section10_text = total_string[section10_start:section11_start]
  section10_revised = clean_text(section10_text, cleanup_string)
  json10 = {'section10':section10_revised}

  # Section 11
  section11_text = total_string[section11_start:section12_start]
  section11_revised = clean_text(section11_text, cleanup_string)
  json11 = {'section11':section11_revised}

  # Section 12
  section12_text = total_string[section12_start:section13_start]
  section12_revised = clean_text(section12_text, cleanup_string)
  json12 = {'section12':section12_revised}

  # Section 13
  section13_text = total_string[section13_start:section14_start]
  section13_revised = clean_text(section13_text, cleanup_string)
  json13 = {'section13':section13_revised}

  # Section 14
  section14_text = total_string[section14_start:section15_start]
  section14_revised = clean_text(section14_text, cleanup_string)
  json14 = {'section14':section14_revised}

  # Section 15
  section15_text = total_string[section15_start:section16_start]
  section15_revised = clean_text(section15_text, cleanup_string)
  json15 = {'section15':section15_revised}

  # Section 16
  section16_text = total_string[section16_start:]
  section16_revised = clean_text(section16_text, cleanup_string)
  json16 = {'section16':section16_revised}

  #print ("completed" + directory)

  return json1, json2, json3, json4, json5, json6, json7, json8, json9, json10, json11, json12, json13, json14, json15, json16

"""Here is an example of how to call the above function. We can change the msds_filename and cleanup_string name depending on what file are we trying to execute. We will just need section 3 of the MSDS file further working on this project. """

if __name__ == '__main__':
  msds_filename = "MSDS_TF400_EN.pdf"
  cleanup_string = "Material Safety Data Sheet  (MSDS)"
  output = myfunction1(msds_filename, cleanup_string)
Section_3 = output[2] #Section 3 of the msds file saved as a .json file 
Section_3

"""We need to make sure to save the file in our google drive. """

from google.colab import drive
drive.mount('/content/drive/')

"""Phase 2:

In phase 2 we will input the section 3 of the MSDS file saved as a json file and extract all the cas numbers that are given in section3.
"""

def myfunction2(section_name):
  import json #importing jspon to extract information from a json file
  import re
  string_1 = json.dumps(section_name)
  string_2 = string_1.replace(' ', '')
  y = re.findall('([0-9]+?)-([0-9]+?)-([0-9]+?)', string_2) #using regex (regular expressions) to find all cas no. in the string

  return y

"""Here is how we call the above function. The output will be in a tuple inside a list format. Section_3 is the output from phase 1, it will change depending on the different MSDS file that we will have."""

Cas_No = myfunction2(Section_3)
Cas_No

"""The below given function just converts the output from Phase 2 in a list of the original cas no. format. Since, in Phase 2 you have them inside a tuple."""

def convert_cas(List):
    new_list = []
    y = ""
    for i in List:
        for j in range(0, len(i)-1):
            x  = i[0]
            y = x + "-" + i[1] + "-" + i[2]
        new_list.append(y)
    return new_list

"""The cas_no_list will be an input for Phase 3."""

cas_no_list = convert_cas(Cas_No)
cas_no_list

"""Phase 3:

In phase 3 the input will be a list of cas numbers that we find from phase 2. 
We will then check if any of the cas number from the list is there in the '2022riskfactors.xlsx' excel file. 
We will do this by using the python pandas library to manipulate the excel file.
This excel file contains the chemical names and cas numbers of the substances that are harmful.
"""

import pandas as pd #importing the python pandas library
def my_function3(cas_no): #The input parameter cas_no will be a list of cas_no
    df = pd.read_excel('2022riskfactors.xlsx') #reading the excel file
    df = df.drop(labels=[0, 1, 2], axis=0)
    new_header = df.iloc[0]
    df = df[1:]
    df.columns = new_header

    new_list = [] #creating an empty list to add any harmful chemicals found
    for i in cas_no:
      if i in df['CAS No.'].values:
          new_list.append(i)
          return new_list
      else:
          print("No harmful chemical found!")

from google.colab import drive
drive.mount('/content/drive')

"""Here is how we call the above function with input as output from phase 2."""

cas_filter = my_function3(cas_no_list)
cas_filter

"""Phase 4: (optional)

In Phase we are creating a database that includes the Cas no. and the Chemical name. We will be extracting text from a webpage that includes information about the Cas no. and its chemical name. We will then create a dictionary with cas no. as the key and the chemical name as values. Finally, we will save it as a .json file.
"""

from urllib.request import urlopen
from bs4 import BeautifulSoup

# new_text = ""

text_1 = ""

for i in range(1, 100):
    url = "http://www.chemnet.com/cas/kr/wikipedia--"+str(i)+".html"
    html = urlopen(url).read()
    soup = BeautifulSoup(html, features="html.parser")

    # kill all script and style elements
    for script in soup(["script", "style"]):
        script.extract()    # rip it out

    # get text
    text = soup.get_text()

    # break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    # drop blank lines
    text = '\n'.join(chunk for chunk in chunks if chunk)
    
    
    #new_text = new_text + text

    x = text.find("CAS상품명칭")
    new_text = text[x:]
    y = new_text.find("World Wide ChemNet")
    renewed_text = new_text[:y].strip("CAS상품명칭")

    text_1 = text_1 + renewed_text
    
    text_1 = text_1.replace('\n\n', '\n').replace('Wolfberry\nextract [11-40-5]', 'Wolfberry extract [11-40-5]')
    text_1 = text_1.replace('2-(4-Hydroxyphenyl)ethylamine\n[51-67-2]', '2-(4-Hydroxyphenyl)ethylamine [51-67-2]')
    text_1 = text_1.replace('Acetonitrile\n[75-05-8]', 'Acetonitrile [75-05-8]')
    text_1 = text_1.replace('N-ethyl-4-methylbenzenesulfonamide\n[80-39-7]', 'N-ethyl-4-methylbenzenesulfonamide [80-39-7]')
    text_1 = text_1.replace("2'-Benzoylacetanilide\n[85-99-4]", "2'-Benzoylacetanilide [85-99-4]")
    text_1 = text_1.replace('6-methyl-quinolin\n[91-62-3]', '6-methyl-quinolin [91-62-3]')
    text_1 = text_1.replace('Dodecyl diethanol amide\n= Lauramide DEA [120-40-1]', 'Dodecyl diethanol amide = Lauramide DEA [120-40-1]')
    text_1 = text_1.replace('Vat Blue\n6 [130-20-1]', 'Vat Blue [130-20-1]')
    text_1 = text_1.replace('Dodecylethanol amide\n= Lauramide MEA [142-78-9]', 'Dodecylethanol amide = Lauramide MEA [142-78-9]')
    text_1 = text_1.replace('Bis(4-fluorophenyl) acetic acid\n[361-63-7]', 'Bis(4-fluorophenyl) acetic acid [361-63-7]')
    text_1 = text_1.replace('alpha-Linolenic acid\n[463-40-1]', 'alpha-Linolenic acid [463-40-1]')
    text_1 = text_1.replace('Solvent Violet\n9 [467-63-0]', 'Solvent Violet 9 [467-63-0]')
    text_1 = text_1.replace('Triprolidine hydrochloride\n[550-70-9]', 'Triprolidine hydrochloride [550-70-9]')
    text_1 = text_1.replace("Adenosine-2',3'-cyclic phosphate,\nDicyclohexylguanidinium Salt [634-01-5]", "Adenosine-2',3'-cyclic phosphate, Dicyclohexylguanidinium Salt [634-01-5]")
    text_1 = text_1.replace('2-fluoro-3-oxopentanoic acid ethylester\n[759-67-1]', '2-fluoro-3-oxopentanoic acid ethylester [759-67-1]')
    text_1 = text_1.replace('5,6-Difluoroisatin\n[774-47-0]', '5,6-Difluoroisatin [774-47-0]')

print(text_1)

new_text_1 = text_1.split('\n')
new_text_1.pop(0)
cas_list_1 = []
chemical_list_1 = []
for i in range(0, len(new_text_1)-1, 2):
    cas_list_1.append(new_text_1[i])
    chemical_list_1.append(new_text_1[i+1])
print(cas_list_1)
print(chemical_list_1)

from urllib.request import urlopen
from bs4 import BeautifulSoup

# new_text = ""

text_2 = ""

for i in range(100, 200):
    url = "http://www.chemnet.com/cas/kr/wikipedia--"+str(i)+".html"
    html = urlopen(url).read()
    soup = BeautifulSoup(html, features="html.parser")

    # kill all script and style elements
    for script in soup(["script", "style"]):
        script.extract()    # rip it out

    # get text
    text = soup.get_text()

    # break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    # drop blank lines
    text = '\n'.join(chunk for chunk in chunks if chunk)
    
    
    #new_text = new_text + text

    x = text.find("CAS상품명칭")
    new_text = text[x:]
    y = new_text.find("World Wide ChemNet")
    renewed_text = new_text[:y].strip("CAS상품명칭")

    text_2 = text_2 + renewed_text
    
text_2 = text_2.replace('\n\n', '\n')
text_2 = text_2.replace('1,1-Bis(4-hydroxyphenyl)cyclohexane\n[843-55-0]', '1,1-Bis(4-hydroxyphenyl)cyclohexane [843-55-0]')
text_2 = text_2.replace('Pigment Red\n49:2 [1103-39-5]', 'Pigment Red 49:2 [1103-39-5]')
text_2 = text_2.replace('4-Phenylthio Benzyl Chloride\n[1208-87-3]', '4-Phenylthio Benzyl Chloride [1208-87-3]')
text_2 = text_2.replace('2-Bromophenyl\nisothiocyanate [1303-76-0]', '2-Bromophenyl isothiocyanate [1303-76-0]')
text_2 = text_2.replace('N-ethyl-4-methylbenzenesulfonamide\n[1321-54-6]', 'N-ethyl-4-methylbenzenesulfonamide [1321-54-6]')
text_2 = text_2.replace('Vat Orange\n2 [1324-35-2]', 'Vat Orange 2 [1324-35-2]')
text_2 = text_2.replace('Direct Orange\n15 [1325-35-5]', 'Direct Orange 15 [1325-35-5]')
text_2 = text_2.replace('2-(4-iodophenyl)acetic acid\n[1798-06-7]', '2-(4-iodophenyl)acetic acid [1798-06-7]')
text_2 = text_2.replace('2-(5-methyl-1H-indol-3-yl)ethanamine\n[1821-47-2]', '2-(5-methyl-1H-indol-3-yl)ethanamine [1821-47-2]')
  
    
print(text_2)

text_2 = text_2.replace('3,4-Dihydroxythiophene-2,5-dicarboxylic acid diethyl ester\n[1822-66-8]', '3,4-Dihydroxythiophene-2,5-dicarboxylic acid diethyl ester [1822-66-8]')
text_2 = text_2.replace('Suberic\ndihydrazide [2024-78-4]', 'Suberic dihydrazide [2024-78-4]')
text_2 = text_2.replace("2'-Deoxycytidine-5'-triphosphoric acid\n= dCTP [2056-98-6]", "2'-Deoxycytidine-5'-triphosphoric acid = dCTP [2056-98-6]")
text_2 = text_2.replace('Vat Orange\n11 [2172-33-0]', 'Vat Orange 11 [2172-33-0]')
text_2 = text_2.replace('N(epsilon)-boc-N(alpha)-Z-L-lysine\ndi cyclohexylaminesalt [2212-76-2]', 'N(epsilon)-boc-N(alpha)-Z-L-lysine di cyclohexylaminesalt [2212-76-2]')
text_2 = text_2.replace('Carbamimidothioic acid, methyl ester, sulfate\n[2260-00-6]', 'Carbamimidothioic acid, methyl ester, sulfate [2260-00-6]')
text_2 = text_2.replace('Vat Yellow\n10 [2379-76-2]', 'Vat Yellow 10 [2379-76-2]')
text_2 = text_2.replace('Pimelic\ndihydrazide [1304-39-8]', 'Pimelic dihydrazide [1304-39-8]')
text_2 = text_2.replace('Glycine\nn-propyl\nester\nhydrochloride [1304-90-1]', 'Glycine n-propyl ester hydrochloride [1304-90-1]')
text_2 = text_2.replace('Sulphur Yellow\n9 [1326-40-5]', 'Sulphur Yellow 9 [1326-40-5]')
text_2 = text_2.replace('Sulphur Black\n2 [1326-85-8]', 'Sulphur Black 2 [1326-85-8]')
text_2 = text_2.replace('6-hydroxynaphthalene-1-carboxylic acid\n[2437-17-4]', '6-hydroxynaphthalene-1-carboxylic acid [2437-17-4]')
text_2 = text_2.replace('Solvent Yellow\n44 [2478-20-8]', 'Solvent Yellow 44 [2478-20-8]')
text_2 = text_2.replace('Direct Blue\n78 [2503-73-3]', 'Direct Blue 78 [2503-73-3]')
text_2 = text_2.replace('3,5-dimethylbenzyl chloride\n[2745-54-2]', '3,5-dimethylbenzyl chloride [2745-54-2]')
text_2 = text_2.replace('2-(N-isopropyl-N-methylamino)-ethanol\n[2893-49-4]', '2-(N-isopropyl-N-methylamino)-ethanol [2893-49-4]')
text_2 = text_2.replace('Basic Red\n1:1 [3068-39-1]', 'Basic Red 1:1 [3068-39-1]')
text_2 = text_2.replace('Vat Green\n3 [3271-76-9]', 'Vat Green 3 [3271-76-9]')
text_2 = text_2.replace('Fluorescent brightener\n134 [3426-43-5]', 'Fluorescent brightener 134 [3426-43-5]')
text_2 = text_2.replace('3,5-dibromo-2,6-dimethylpyridine\n[3430-34-0]', '3,5-dibromo-2,6-dimethylpyridine [3430-34-0]')
text_2 = text_2.replace('(E)-6,10-Dimethyl-5,9-undecadien-2-one\n[3796-70-1]', '(E)-6,10-Dimethyl-5,9-undecadien-2-one [3796-70-1]')
text_2 = text_2.replace('Acid Brown\n2 [3626-41-3]', 'Acid Brown 2 [3626-41-3]')
text_2 = text_2.replace('4-Bromo-9H-carbazole\n[3652-89-9]', '4-Bromo-9H-carbazole [3652-89-9]')
text_2 = text_2.replace('2,4-Dimethylphenyl\nisothiocyanate [3984-20-1]', '2,4-Dimethylphenyl isothiocyanate [3984-20-1]')
text_2 = text_2.replace('p-Xylylenebis(triphenylphosphonium\nbromide) [4081-70-3]', 'p-Xylylenebis(triphenylphosphonium bromide) [4081-70-3]')
text_2 = text_2.replace('Pigment Yellow\n6 [4106-76-7]', 'Pigment Yellow 6 [4106-76-7]')
text_2 = text_2.replace('1-chloro-3-ethoxy-2-propanol\n[4151-98-8]', '1-chloro-3-ethoxy-2-propanol [4151-98-8]')
text_2 = text_2.replace('4-Cyanotetrahydropyran\n[4295-99-2]', '4-Cyanotetrahydropyran [4295-99-2]')
text_2 = text_2.replace('Acid Blue\n62 [4368-56-3]', 'Acid Blue 62 [4368-56-3]')
text_2 = text_2.replace('Vat Red\n13 [4203-77-4]', 'Vat Red 13 [4203-77-4]')
text_2 = text_2.replace('Iodomethyltrimethylsilane\n= (Iodomethyl)-trimethylsilane [4206-67-1]', 'Iodomethyltrimethylsilane = (Iodomethyl)-trimethylsilane [4206-67-1]')
text_2 = text_2.replace('Acid Blue\n47 [4403-89-8]', 'Acid Blue 47 [4403-89-8]')
print(text_2)

new_text_2 = text_2.split('\n')
new_text_2.pop(0)
cas_list_2 = []
chemical_list_2 = []
for i in range(0, len(new_text_2)-1, 2):
    cas_list_2.append(new_text_2[i])
    chemical_list_2.append(new_text_2[i+1])
print(cas_list_2)
print(chemical_list_2)

from urllib.request import urlopen
from bs4 import BeautifulSoup

# new_text = ""

text_3 = ""

for i in range(200, 300):
    url = "http://www.chemnet.com/cas/kr/wikipedia--"+str(i)+".html"
    html = urlopen(url).read()
    soup = BeautifulSoup(html, features="html.parser")

    # kill all script and style elements
    for script in soup(["script", "style"]):
        script.extract()    # rip it out

    # get text
    text = soup.get_text()

    # break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    # drop blank lines
    text = '\n'.join(chunk for chunk in chunks if chunk)
    
    
    #new_text = new_text + text

    x = text.find("CAS상품명칭")
    new_text = text[x:]
    y = new_text.find("World Wide ChemNet")
    renewed_text = new_text[:y].strip("CAS상품명칭")

    text_3 = text_3 + renewed_text
    
    text_3 = text_3.replace('\n\n', '\n')
    
print(text_3)

text_3 = text_3.replace('Acid Red\n80 [4478-76-6]', 'Acid Red 80 [4478-76-6]')
text_3 = text_3.replace('Pigment Yellow\n17 [4531-49-1]', 'Pigment Yellow 17 [4531-49-1]')
text_3 = text_3.replace('Solvent Yellow\n93 [4702-90-3]', 'Solvent Yellow 93 [4702-90-3]')
text_3 = text_3.replace('trimethylsulfoxonium\nchloride [5034-06-0]', 'trimethylsulfoxonium chloride [5034-06-0]')
text_3 = text_3.replace('Pigment Yellow\n13 [5102-83-0]', 'Pigment Yellow 13 [5102-83-0]')
text_3 = text_3.replace('Pigment Red\n48:4 [5280-66-0]', 'Pigment Red 48:4 [5280-66-0]')
text_3 = text_3.replace('3-Bromo-8-nitroquinoline\n[5341-07-1]', '3-Bromo-8-nitroquinoline [5341-07-1]')
text_3 = text_3.replace('3-Pyrrolidinemethanol, 1-(phenylmethyl)-\n[5731-17-9]', '3-Pyrrolidinemethanol, 1-(phenylmethyl)- [5731-17-9]')
text_3 = text_3.replace('Vat Black\n29 [6049-19-0]', 'Vat Black 29 [6049-19-0]')
text_3 = text_3.replace('Acid Black\n26 [6262-07-3]', 'Acid Black 26 [6262-07-3]')
text_3 = text_3.replace('Acid Red\n6 [6245-59-6]', 'Acid Red 6 [6245-59-6]')
text_3 = text_3.replace('Acid Red\n42 [6245-60-9]', 'Acid Red 42 [6245-60-9]')
text_3 = text_3.replace('Benzamide, 4-amino-N-methyl- (9CI)\n[6274-22-2]', 'Benzamide, 4-amino-N-methyl- (9CI) [6274-22-2]')
text_3 = text_3.replace('Basic Red\n12 [6320-14-5]', 'Basic Red 12 [6320-14-5]')
text_3 = text_3.replace('Direct Green\n26 [6388-26-7]', 'Direct Green 26 [6388-26-7]')
text_3 = text_3.replace('Pigment Yellow\n55 [6358-37-8]', 'Pigment Yellow 55 [6358-37-8]')
text_3 = text_3.replace('Acid Brown\n15 [5850-15-7]', 'Acid Brown 15 [5850-15-7]')
text_3 = text_3.replace('Acid Green\n20 [5850-39-5]', 'Acid Green 20 [5850-39-5]')
text_3 = text_3.replace('Acid Orange\n3 [6373-74-6]', 'Acid Orange 3 [6373-74-6]')
text_3 = text_3.replace('Direct Blue\n75 [6428-60-0]', 'Direct Blue 75 [6428-60-0]')
text_3 = text_3.replace('Acid Brown\n4 [5858-51-5]', 'Acid Brown 4 [5858-51-5]')
text_3 = text_3.replace('Acid Blue\n15 [5863-46-7]', 'Acid Blue 15 [5863-46-7]')
text_3 = text_3.replace('Pigment Red\n21 [6410-26-0]', 'Pigment Red 21 [6410-26-0]')
text_3 = text_3.replace('Pigment Red\n63:1 [6417-83-0]', 'Pigment Red 63:1 [6417-83-0]')
text_3 = text_3.replace('Disperse Yellow\n5 [6439-53-8]', 'Disperse Yellow 5 [6439-53-8]')
text_3 = text_3.replace('Acid Violet\n1 [6441-91-4]', 'Acid Violet 1 [6441-91-4]')
text_3 = text_3.replace('Acid Red\n35 [6441-93-6]', 'Acid Red 35 [6441-93-6]')
text_3 = text_3.replace('Solvent Black\n34 [6459-65-0]', 'Solvent Black 34 [6459-65-0]')
# text_3 = text_3.replace('')
# text_3 = text_3.replace('')
# text_3 = text_3.replace('')
# text_3 = text_3.replace('')
# text_3 = text_3.replace('')
# text_3 = text_3.replace('')
# text_3 = text_3.replace('')
# text_3 = text_3.replace('')





print(text_3)

new_text_3 = text_3.split('\n')
new_text_3.pop(0)
cas_list_3 = []
chemical_list_3 = []
for i in range(0, len(new_text_3)-1, 2):
    cas_list_3.append(new_text_3[i])
    chemical_list_3.append(new_text_3[i+1])
print(cas_list_3)
#print(chemical_list_2)

from urllib.request import urlopen
from bs4 import BeautifulSoup

# new_text = ""

text_4 = ""

for i in range(300, 400):
    url = "http://www.chemnet.com/cas/kr/wikipedia--"+str(i)+".html"
    html = urlopen(url).read()
    soup = BeautifulSoup(html, features="html.parser")

    # kill all script and style elements
    for script in soup(["script", "style"]):
        script.extract()    # rip it out

    # get text
    text = soup.get_text()

    # break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    # drop blank lines
    text = '\n'.join(chunk for chunk in chunks if chunk)
    
    
    #new_text = new_text + text

    x = text.find("CAS상품명칭")
    new_text = text[x:]
    y = new_text.find("World Wide ChemNet")
    renewed_text = new_text[:y].strip("CAS상품명칭")

    text_4 = text_4 + renewed_text
    
    text_4 = text_4.replace('\n\n', '\n')
    
print(text_4)

from urllib.request import urlopen
from bs4 import BeautifulSoup

# new_text = ""

text_5 = ""

for i in range(400, 500):
    url = "http://www.chemnet.com/cas/kr/wikipedia--"+str(i)+".html"
    html = urlopen(url).read()
    soup = BeautifulSoup(html, features="html.parser")

    # kill all script and style elements
    for script in soup(["script", "style"]):
        script.extract()    # rip it out

    # get text
    text = soup.get_text()

    # break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    # drop blank lines
    text = '\n'.join(chunk for chunk in chunks if chunk)
    
    
    #new_text = new_text + text

    x = text.find("CAS상품명칭")
    new_text = text[x:]
    y = new_text.find("World Wide ChemNet")
    renewed_text = new_text[:y].strip("CAS상품명칭")

    text_5 = text_5 + renewed_text
    
    # text_1 = text_1.replace('\n\n', '').replace('Wolfberry\nextract [11-40-5]', 'Wolfberry extract [11-40-5]')
    
print(text_5)

final_cas_list = cas_list_1 + cas_list_2 + cas_list_3 + cas_list_4 + cas_list_5 + cas_list_6 + cas_list_7 + cas_list_8 
final_chemical_list = chemical_list_1 + chemical_list_2 + chemical_list_3 + chemical_list_4 + chemical_list_5 + chemical_list_6 + chemical_list_7 + chemical_list_8

cas_chemical_database = {}
for key in final_cas_list:
  for value in final_chemical_list:
    cas_chemical_database[key] = value
    break